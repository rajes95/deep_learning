{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CS7140/PA-7/blob/main/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBf9Od1gcrhh"
   },
   "source": [
    "Rajesh Sakhamuru\n",
    "\n",
    "11-7-2020\n",
    "# GoogLeNet Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ps3KnoYRNxM8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMXc2PQ7c3CS",
    "outputId": "4f79c839-e474-46aa-e718-6b278d9e61dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1r8qA3Eyc8Sz"
   },
   "outputs": [],
   "source": [
    "class inceptionBlock(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n1, n2, n3, n4):\n",
    "        super().__init__()\n",
    "        self.branch1 = tf.keras.layers.Conv2D(n1, kernel_size=1, activation='relu')\n",
    "\n",
    "        self.branch2 = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Conv2D(n2[0], 1, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(n2[1], 3, padding='same', activation='relu')]) \n",
    "        \n",
    "        self.branch3 = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Conv2D(n3[0], 1, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(n3[1], 5, padding='same', activation='relu')]) \n",
    "\n",
    "        self.branch4 = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.MaxPool2D(3, 1, padding='same'),\n",
    "                tf.keras.layers.Conv2D(n4, 1, activation='relu')]) \n",
    "        \n",
    "    def call(self, X):\n",
    "        return tf.keras.layers.Concatenate()([self.branch1(X),self.branch2(X),self.branch3(X),self.branch4(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mqriWpV5JXSn"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "    train_labels = tf.convert_to_tensor(train_labels)\n",
    "    test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "    train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "    test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
    "\n",
    "    # resizes images and pads resized image with 0s so convolutions+pooling don't shrink\n",
    "    # resolution too much\n",
    "    train_images = tf.image.resize_with_pad(train_images, 96, 96)\n",
    "    test_images = tf.image.resize_with_pad(test_images, 96, 96)\n",
    "\n",
    "    train_images = tf.convert_to_tensor((train_images) / 255)\n",
    "    test_images = tf.convert_to_tensor((test_images) / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXNmZoO8Kdri",
    "outputId": "799594e8-3dd5-4a60-a443-2aa6d09b7f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "938/938 - 46s - loss: 2.0254 - accuracy: 0.2891 - val_loss: 1.9307 - val_accuracy: 0.3842\n",
      "Epoch 2/15\n",
      "938/938 - 44s - loss: 0.7287 - accuracy: 0.7129 - val_loss: 0.6991 - val_accuracy: 0.7354\n",
      "Epoch 3/15\n",
      "938/938 - 44s - loss: 0.5134 - accuracy: 0.8059 - val_loss: 0.7775 - val_accuracy: 0.7070\n",
      "Epoch 4/15\n",
      "938/938 - 44s - loss: 0.4324 - accuracy: 0.8382 - val_loss: 0.4823 - val_accuracy: 0.8154\n",
      "Epoch 5/15\n",
      "938/938 - 44s - loss: 0.3834 - accuracy: 0.8571 - val_loss: 0.4184 - val_accuracy: 0.8525\n",
      "Epoch 6/15\n",
      "938/938 - 44s - loss: 0.3516 - accuracy: 0.8694 - val_loss: 0.4246 - val_accuracy: 0.8371\n",
      "Epoch 7/15\n",
      "938/938 - 44s - loss: 0.3251 - accuracy: 0.8781 - val_loss: 0.3367 - val_accuracy: 0.8743\n",
      "Epoch 8/15\n",
      "938/938 - 44s - loss: 0.3042 - accuracy: 0.8862 - val_loss: 0.3360 - val_accuracy: 0.8717\n",
      "Epoch 9/15\n",
      "938/938 - 44s - loss: 0.2861 - accuracy: 0.8935 - val_loss: 0.3132 - val_accuracy: 0.8844\n",
      "Epoch 10/15\n",
      "938/938 - 44s - loss: 0.2722 - accuracy: 0.8977 - val_loss: 0.3500 - val_accuracy: 0.8712\n",
      "Epoch 11/15\n",
      "938/938 - 44s - loss: 0.2591 - accuracy: 0.9023 - val_loss: 0.3290 - val_accuracy: 0.8791\n",
      "Epoch 12/15\n",
      "938/938 - 44s - loss: 0.2475 - accuracy: 0.9068 - val_loss: 0.2924 - val_accuracy: 0.8901\n",
      "Epoch 13/15\n",
      "938/938 - 44s - loss: 0.2369 - accuracy: 0.9116 - val_loss: 0.3288 - val_accuracy: 0.8744\n",
      "Epoch 14/15\n",
      "938/938 - 44s - loss: 0.2290 - accuracy: 0.9140 - val_loss: 0.2679 - val_accuracy: 0.9043\n",
      "Epoch 15/15\n",
      "938/938 - 44s - loss: 0.2196 - accuracy: 0.9167 - val_loss: 0.2630 - val_accuracy: 0.9059\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    googLeNet = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, 7, strides=2, padding='same', activation='relu', input_shape=(96, 96, 1)),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.Conv2D(64, 1, activation='relu'),\n",
    "            tf.keras.layers.Conv2D(192, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "            inceptionBlock(64, (96, 128), (16, 32), 32),\n",
    "            inceptionBlock(128, (128, 192), (32, 96), 64), \n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "            inceptionBlock(64, (96, 128), (16, 32), 32),\n",
    "            inceptionBlock(128, (128, 192), (32, 96), 64), \n",
    "            inceptionBlock(128, (128, 192), (32, 96), 64),\n",
    "            inceptionBlock(128, (128, 192), (32, 96), 64), \n",
    "            inceptionBlock(256, (160, 320), (32, 128), 128), \n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "            inceptionBlock(64, (96, 128), (16, 32), 32),\n",
    "            inceptionBlock(128, (128, 192), (32, 96), 64), \n",
    "            tf.keras.layers.GlobalAvgPool2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10)])\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    googLeNet.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    googLeNetHistory = googLeNet.fit(train_images, train_labels, epochs=15, batch_size=64, validation_data=(test_images, test_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLsq9CKUYXxF"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The number of model parameters for GoogLeNet is 3,749,146 (as can be seen below) which is an order of magnitude lower than the number of model parameters for VGG-11 which had 22,121,802 parameters and AlexNet which had 21,598,922 parameters for similarly performing networks. That noted, VGG-11 still performed ~1.5% better than GoogLeNet and AlexNet.\n",
    "\n",
    "The reason GoogLeNet is able to significantly reduce model parameter size is because the Inception layer used 9 times within the architecture of GoogLeNet uses convolutions of different sizes (5x5, 3x3, 1x1) to capture detail within the images. Particularly the 1x1 convolutions within the inception layers allow to significantly reduce the dimension of data input into the 5x5 and 3x3 convolutions and avoid the blowing up of dimensions as the network gets deeper. \n",
    "\n",
    "Also, by replacing the fully connected layers of AlexNet and VGG, GoogLeNet instead uses a global average pooling layer after the last convolutional layer. This additionally drastically reduces the number of parameters in the network because the image recognition features are drawn out and mapped in the convolutional layers instead in GoogLeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHeRf2THJk0q",
    "outputId": "11447a0f-0269-4873-f8ad-632359db2972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_227\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_411 (Conv2D)          (None, 48, 48, 64)        3200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_412 (Conv2D)          (None, 24, 24, 64)        4160      \n",
      "_________________________________________________________________\n",
      "conv2d_413 (Conv2D)          (None, 24, 24, 192)       110784    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling (None, 12, 12, 192)       0         \n",
      "_________________________________________________________________\n",
      "inception_block_63 (inceptio (None, 12, 12, 256)       163696    \n",
      "_________________________________________________________________\n",
      "inception_block_64 (inceptio (None, 12, 12, 480)       388736    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_99 (MaxPooling (None, 6, 6, 480)         0         \n",
      "_________________________________________________________________\n",
      "inception_block_65 (inceptio (None, 6, 6, 256)         223600    \n",
      "_________________________________________________________________\n",
      "inception_block_66 (inceptio (None, 6, 6, 480)         388736    \n",
      "_________________________________________________________________\n",
      "inception_block_67 (inceptio (None, 6, 6, 480)         467584    \n",
      "_________________________________________________________________\n",
      "inception_block_68 (inceptio (None, 6, 6, 480)         467584    \n",
      "_________________________________________________________________\n",
      "inception_block_69 (inceptio (None, 6, 6, 832)         840704    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_105 (MaxPoolin (None, 3, 3, 832)         0         \n",
      "_________________________________________________________________\n",
      "inception_block_70 (inceptio (None, 3, 3, 256)         296816    \n",
      "_________________________________________________________________\n",
      "inception_block_71 (inceptio (None, 3, 3, 480)         388736    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                4810      \n",
      "=================================================================\n",
      "Total params: 3,749,146\n",
      "Trainable params: 3,749,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "googLeNet.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Q3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
