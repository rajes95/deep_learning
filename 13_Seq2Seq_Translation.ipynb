{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CS7140/PA-10/blob/main/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o0C1-beObW4",
    "outputId": "ba1c7478-18ea-4a1a-8394-e610edc41939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.44 ms\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l==0.15.1\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkEFhdtLw3NI"
   },
   "source": [
    "Rajesh Sakhamuru\n",
    "\n",
    "12-9-2020\n",
    "# Encoder-Decoder Seq2Seq for English to French Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAHcKx40yo_I"
   },
   "source": [
    "The encoder and decoder do not have to be the same type of neural network. There are many potential uses where encoder-decoder architecture can be useful. One such example that shows that the neural network types do not have to be the same is when this encoder-decoder architecture is used for image captioning. The data being encoded is an image via a Convolutional Neural Network (CNN) but the decoder neural network would be a Recurrent Neural Network (RNN) which would use the state output of the CNN as input to generate a text caption for the image.\n",
    "\n",
    "If the encoder and decoder differ in the number of layers or the number of hidden units, the decoder hidden state can be initialized by the encoder hidden state output with every decoder layer. This way, the alignment of the encoder state with the decoder state is not a problem because this method allows the decoder to learn which parts of the input sequence are relevant to each part of the output sequence.\n",
    "\n",
    "Another solution would be to use an attention model which would develop a context vector which is specifically filtered for each output time-step. Attention models achieve similar results to feeding the hidden state to each decoder layer but much more effeciently and explicitly makes the selection of which parts input sequence are relevant to which parts of the output sequence.\n",
    "\n",
    "Below I have my implementation of Seq2Seq which is an Encoder-Decoder neural network for translating English to French based on the D2L implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1EiZ-d5uKJ3",
    "outputId": "742da121-d603-47a9-ba1a-2f3165e4c15f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 574 ms\n"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "import collections\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8GACmyRYwL6",
    "outputId": "9b0dbe99-7a98-4bc5-f140-07a8ff50af40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.51 ms\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, hiddenSize, numLayers, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.embed = nn.Embedding(vocabSize, embedSize)\n",
    "        self.gru = nn.GRU(embedSize, hiddenSize, num_layers=numLayers, dropout=0.2)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        embedded = self.embed(X)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        output, hiddenState = self.gru(embedded)\n",
    "        return output, hiddenState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1dYJT6FamnM",
    "outputId": "c7091d70-770d-4aad-f709-c74d0dfd605f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.8 ms\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, hiddenSize, numLayers, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.embed = nn.Embedding(vocabSize, embedSize)\n",
    "        self.gru = nn.GRU(embedSize + hiddenSize, hiddenSize, numLayers, dropout=0.2)\n",
    "        self.dense = nn.Linear(hiddenSize, vocabSize)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        embedded = self.embed(X)\n",
    "        embedded = embedded.permute(1,0,2)\n",
    "        context = state[-1].repeat(embedded.shape[0], 1, 1)\n",
    "        X_context = torch.cat((embedded, context), 2)\n",
    "        output, hiddenState = self.gru(X_context, state)\n",
    "        output = self.dense(output)\n",
    "        output = output.permute(1,0,2)\n",
    "        return output, hiddenState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeZcWWqxcXzY",
    "outputId": "270942ef-88a2-4c53-fcb7-b4c49d661cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.12 ms\n"
     ]
    }
   ],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFQVBAjO-TL7",
    "outputId": "c584562c-4c94-432b-9485-fb1a1cbdaec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "class EncoderDecoderLoss(nn.CrossEntropyLoss):\n",
    "    def sequenceMask(self, weights, validLen, value=0):\n",
    "        maxlen = weights.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=weights.device)[None, :] < validLen[:, None]\n",
    "        weights[~mask] = value\n",
    "        return weights\n",
    "\n",
    "    def forward(self, pred, label, validLen):\n",
    "        self.reduction='none'\n",
    "        unweightedLoss = super(EncoderDecoderLoss, self).forward(pred.permute(0,2,1), label)\n",
    "\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = self.sequenceMask(weights, validLen)\n",
    "    \n",
    "        weightedLoss = (unweightedLoss * weights)\n",
    "        return weightedLoss.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSbeWmTJHx3G",
    "outputId": "7d0edb84-0935-40f9-e5eb-e97e89749739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.59 ms\n"
     ]
    }
   ],
   "source": [
    "def initializeWeights(model):\n",
    "    if type(model) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(model.weight)\n",
    "    if type(model) == nn.GRU:\n",
    "        for param in model._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                torch.nn.init.xavier_uniform_(model._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Luab2PWZVkk",
    "outputId": "c84ddf52-d03c-46d1-9593-32e0927929b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.62 ms\n"
     ]
    }
   ],
   "source": [
    "def gradClipping(model, theta):\n",
    "    if isinstance(model, nn.Module):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = model.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hUQ8fi5FcBt",
    "outputId": "b0badccc-0e1e-40a8-b029-7a82451517bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.7 ms\n"
     ]
    }
   ],
   "source": [
    "def trainModel(model, dataIter, lr, numEpochs, vocabList, device):\n",
    "    model.apply(initializeWeights)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lossFunc = EncoderDecoderLoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        perplexity = [0.0] * 2\n",
    "        for batch in dataIter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([vocabList['<bos>']]*Y.shape[0], device=device)\n",
    "            bos = bos.reshape(-1,1)\n",
    "            decInput = torch.cat([bos, Y[:,:-1]], 1)\n",
    "\n",
    "            preds, _ = model(X, decInput, X_valid_len)\n",
    "            loss = lossFunc(preds, Y, Y_valid_len)\n",
    "            loss.sum().backward()\n",
    "            gradClipping(model, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                perplexity = [a + float(b) for a, b in zip(perplexity, [loss.sum(), num_tokens])]\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "                print('epoch:',epoch+1,'|| loss:',loss.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "362Tw1viUfKc",
    "outputId": "f4f0885d-e9f0-47c7-a240-18020a6b8388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "def predictTranslation(model, eng, engVocab, freVocab, numSteps, device):\n",
    "    src_tokens = engVocab[eng.lower().split(' ')] + [engVocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, numSteps, engVocab['<pad>'])\n",
    "\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = model.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor([freVocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq = []\n",
    "    for _ in range(numSteps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        \n",
    "        # Once the end-of-sequence token is predicted, the generation of\n",
    "        # the output sequence is complete\n",
    "        \n",
    "        if pred == freVocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "\n",
    "    return ' '.join(freVocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK8-Yf739dd7",
    "outputId": "5bbdcb12-830b-4d5e-b578-a9bda1b0bb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.07 ms\n"
     ]
    }
   ],
   "source": [
    "def translate(model, engPhrases, frePhrases, engVocab, freVocab, numSteps, device):\n",
    "    for eng, fra in zip(engPhrases, frePhrases):\n",
    "        translation = predictTranslation(model, eng, engVocab, freVocab, numSteps, device)\n",
    "        print(eng, '=>', translation, '|| actual:', fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZCoU9oyN2-2",
    "outputId": "887f6cf2-4cd9-4560-b858-e35d2ad8da7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 || loss: 8.068324089050293\n",
      "epoch: 100 || loss: 3.3441240787506104\n",
      "epoch: 150 || loss: 3.24617075920105\n",
      "epoch: 200 || loss: 2.3754284381866455\n",
      "epoch: 250 || loss: 3.188391923904419\n",
      "epoch: 300 || loss: 3.27647066116333\n",
      "epoch: 350 || loss: 1.7688812017440796\n",
      "time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "embedSize, hiddenSize, numLayers = 32, 32, 2\n",
    "lr, numEpochs = 0.005, 350\n",
    "\n",
    "trainIterator, engVocab, freVocab = d2l.load_data_nmt(batch_size=64, num_steps=10)\n",
    "\n",
    "encoder = Encoder(len(engVocab), embedSize, hiddenSize, numLayers)\n",
    "decoder = Decoder(len(freVocab), embedSize, hiddenSize, numLayers)\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "trainModel(model, trainIterator, lr, numEpochs, freVocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHcPNQbfUobH",
    "outputId": "161bf5bf-1ba0-47c7-b7a8-9ca935c055f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va ! || actual: va !\n",
      "i lost . => j'ai perdu perdu . || actual: j'ai perdu .\n",
      "i'm home . => je suis chez moi la <unk> ! || actual: je suis chez moi .\n",
      "he's calm . => il est perdu bon . || actual: il est calme .\n",
      "time: 39.6 ms\n"
     ]
    }
   ],
   "source": [
    "engPhrases = ['go .', \"i lost .\", 'i\\'m home .', 'he\\'s calm .']\n",
    "frePhrases = ['va !', 'j\\'ai perdu .', 'je suis chez moi .', 'il est calme .']\n",
    "translate(model, engPhrases, frePhrases, engVocab, freVocab, numSteps=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Q3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
