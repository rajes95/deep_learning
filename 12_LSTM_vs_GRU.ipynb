{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmDXVr5mkFLt"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==0.15.1\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3eMV6bVjvr7"
   },
   "source": [
    "Rajesh Sakhamuru\n",
    "\n",
    "12-8-2020\n",
    "# LSTM vs GRU for Next Char Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLT-SxEQt6Yv",
    "outputId": "bcfbd0fc-04ae-4334-e6cb-32b7122a7753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 450 ms\n"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3SvSQxvnboL",
    "outputId": "82b70277-d2bb-48d1-f07c-db5a4d4fe040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.7 ms\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocabSize, numHiddens, numLayers, **kwargs):\n",
    "        super(LSTMModel, self).__init__(**kwargs)\n",
    "        self.lstmLayer = nn.LSTM(input_size=vocabSize,\n",
    "                                 hidden_size=numHiddens,\n",
    "                                 num_layers=numLayers)\n",
    "        self.vocabSize = vocabSize\n",
    "        self.numHiddens = numHiddens\n",
    "\n",
    "        self.dense = nn.Linear(numHiddens, vocabSize)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = F.one_hot(X.T.long(), self.vocabSize)\n",
    "        X = X.to(torch.float32)\n",
    "\n",
    "        output, hiddenState = self.lstmLayer(X, state)\n",
    "        output = output.reshape((-1, output.shape[-1]))\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, hiddenState\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        return (torch.zeros((self.lstmLayer.num_layers, batch_size, self.numHiddens), device=device), \n",
    "                torch.zeros((self.lstmLayer.num_layers, batch_size, self.numHiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ivov5IJmaNh",
    "outputId": "2ba22f01-986f-4676-ea46-e1b69f53aa09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.9 ms\n"
     ]
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocabSize, numHiddens, numLayers, **kwargs):\n",
    "        super(GRUModel, self).__init__(**kwargs)\n",
    "        self.gruLayer = nn.GRU(input_size=vocabSize,\n",
    "                                 hidden_size=numHiddens,\n",
    "                                 num_layers=numLayers)\n",
    "        self.vocabSize = vocabSize\n",
    "        self.numHiddens = numHiddens\n",
    "\n",
    "        self.dense = nn.Linear(numHiddens, vocabSize)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = F.one_hot(X.T.long(), self.vocabSize)\n",
    "        X = X.to(torch.float32)\n",
    "\n",
    "        output, hiddenState = self.gruLayer(X, state)\n",
    "        output = output.reshape((-1, output.shape[-1]))\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, hiddenState\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        return torch.zeros((self.gruLayer.num_layers, batch_size, self.numHiddens), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_T0IABuR1Ak",
    "outputId": "9ecdb8e4-92b6-44f9-d536-e73130bd172e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "def trainEpoch(model, train_iter, loss, optimizer, device):\n",
    "    state = None\n",
    "    perplexity = [0.0] * 2\n",
    "    \n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            state = model.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(state, tuple):\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "            else:\n",
    "                state.detach_()\n",
    "        \n",
    "        y = Y.T.reshape(-1)\n",
    "        y = y.to(device)\n",
    "        X = X.to(device)\n",
    "\n",
    "        pred, state = model(X, state)\n",
    "\n",
    "        l = loss(pred, y.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        perplexity = [a + float(b) for a, b in zip(perplexity, [l.mean()*len(y), len(y)])]\n",
    "    \n",
    "    return math.exp(perplexity[0]/perplexity[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bHNCS7TNbdd",
    "outputId": "9eb912c8-5785-4e9a-a073-0e65d9ff6132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.3 ms\n"
     ]
    }
   ],
   "source": [
    "def predictChars(prefix, numPreds, model, vocab, device):\n",
    "    state = model.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "\n",
    "    for y in prefix[1:]:\n",
    "        _, state = model(torch.reshape(torch.tensor([outputs[-1]], device=device), (1, 1)), state)\n",
    "        outputs.append(vocab[y])\n",
    "\n",
    "    for _ in range(numPreds):\n",
    "        y, state = model(torch.reshape(torch.tensor([outputs[-1]], device=device), (1, 1)), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFy4FIHq_0xI",
    "outputId": "6d18651d-42ca-48b7-be03-56c9049e454a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "vocabSize, numHiddens, numLayers = len(vocab), 256, 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "063uABxlzXrl"
   },
   "source": [
    "## LSTM vs GRU Performance\n",
    "\n",
    "LSTM and GRU here both trained below. LSTM model took ~25sec to train 500 epochs, and GRU took ~19sec to train 500 epochs. GRU takes less time because there are fewer gates (reset and update) versus LSTM with three gates (input, output and forget). These fewer gates makes it more effecient therefore resulting in shorter computation time. \n",
    "\n",
    "Instead of accuracy, the models can be compared using perplexity of the output of the GRU and LSTM models. GRU also reached a lower perplexity score at the 500th epoch, which was very slightly lower than the perplexity score achieved by the LSTM model. From just a subjective point of view we can see that the text produced by the GRU model is a slightly more readable result. The reason it may perform better than LSTM for this type of character-level language model is that the GRU exposes the full hidden content without any control via a memory unit, wheras LSTM by controlling which information is passed to the next time-steps in a character-level model could result in less readable results.\n",
    "\n",
    "All of these trends hold for any seed characters/prefix phrase which is in english. When given a gibberish prefix, the results can be unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7aDPaXY_oYs",
    "outputId": "d991e929-2356-4c4c-9b4d-022339d09b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 : perplexity: 14.015 : story the the the the the the the the the the the the th\n",
      "epoch: 99 : perplexity: 9.25 : story the the the the the the the the the the the the th\n",
      "epoch: 149 : perplexity: 5.767 : story three dimensions of thick and this is a cont that \n",
      "epoch: 199 : perplexity: 1.886 : story this that is the time travellerit s against rearly\n",
      "epoch: 249 : perplexity: 1.075 : story he dige traveller same bery regarded as something \n",
      "epoch: 299 : perplexity: 1.057 : story the time traveller proceeded anyreal body must hav\n",
      "epoch: 349 : perplexity: 1.04 : story thend the lantt at a coal in the fire iftime is re\n",
      "epoch: 399 : perplexity: 1.03 : story then thick sust wither still seeming and theinequa\n",
      "epoch: 449 : perplexity: 1.024 : story there is a pouttar is they couldmaster the perspec\n",
      "epoch: 499 : perplexity: 1.033 : story surioned froly in the butter of matter to bass tht\n",
      "time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocabSize, numHiddens, numLayers).to(device)\n",
    "ceLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2)\n",
    "\n",
    "for epoch in range(500):\n",
    "    perplexity = trainEpoch(model, train_iter, ceLoss, optimizer, device)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        prediction = predictChars('story ', 50, model, vocab, device)\n",
    "        print('epoch:',epoch,': perplexity:',round(perplexity,3) ,\":\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzOvSsnKmlD9",
    "outputId": "75f176cc-2c65-4b5d-d23e-146f8738d1e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 : perplexity: 8.621 : story the the the the the the the the the the the the th\n",
      "epoch: 99 : perplexity: 3.867 : story said the medical man a cube thing seing in and dir\n",
      "epoch: 149 : perplexity: 1.12 : story sime there isno difference between time and any of\n",
      "epoch: 199 : perplexity: 1.052 : story sime there isno difference between time and any of\n",
      "epoch: 249 : perplexity: 1.041 : story sime there were also perhaps a dozen candles about\n",
      "epoch: 299 : perplexity: 1.026 : story sime there were also perhaps a dozen candles about\n",
      "epoch: 349 : perplexity: 1.03 : story sime there were also perhaps a dozen candles about\n",
      "epoch: 399 : perplexity: 1.026 : story side there were also perhaps a dozen candles about\n",
      "epoch: 449 : perplexity: 1.024 : story simonst reith of hand trick or other said the medi\n",
      "epoch: 499 : perplexity: 1.017 : story simonst reason said filbywhat is all right said th\n",
      "time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "model = GRUModel(vocabSize, numHiddens, numLayers).to(device)\n",
    "ceLoss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2)\n",
    "\n",
    "for epoch in range(500):\n",
    "    perplexity = trainEpoch(model, train_iter, ceLoss, optimizer, device)\n",
    "    if (epoch+1)%50 == 0:\n",
    "        prediction = predictChars('story ', 50, model, vocab, device)\n",
    "        print('epoch:',epoch,': perplexity:',round(perplexity,3) ,\":\", prediction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
